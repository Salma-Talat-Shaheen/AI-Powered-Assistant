## 👁️‍🗨️ Vision Through Touch — AI Project for the Visually Impaired  
**Girls in ICT Day — Computer Vision with Deep Learning**

To better understand the project we presented on *Girls in ICT Day*, please take a moment to watch the following video:

---

### 🔹 Project Overview

This project leverages **artificial intelligence and deep learning** in the field of **computer vision** to assist visually impaired individuals in perceiving their surroundings.

The system starts by identifying the user via **fingerprint authentication**, then activates a connected camera to capture snapshots of the user's environment. AI algorithms analyze the images to detect and describe surrounding elements through **audio feedback**.

---

### 🔹 Key Features

1. **Text Recognition & Translation**  
   Detects text in any language and translates it into English.

2. **Object Direction Detection**  
   Identifies the direction of recognized elements relative to the user.

3. **Distance Estimation**  
   Calculates approximate distances between the user and surrounding objects.

---

### 🔹 How It Works

Once the environment is captured, the system:
- Analyzes the scene using deep learning models  
- Converts findings into speech-based outputs  
- Notifies the user of surrounding objects, their direction, and distance  

All outputs are communicated via **audio messages**, making the experience accessible and real-time.

---

### 🔹 Implementation & Future Plans

We simulated the project using a live mobile camera connected to the code, enabling real-time detection and feedback during the demo. Future development plans include:
- Mobile application integration  
- Hardware device with built-in camera and speaker  
- Additional features such as object classification confidence and environment mode switching

---

> This project is one of my proudest contributions to accessible AI. It has received enthusiastic feedback and demonstrates real-world applicability in supporting individuals with visual impairments.

---
